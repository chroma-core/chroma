{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Custom Benchmark\n",
    "\n",
    "This notebook walks through how to generate a custom benchmark based on your data.\n",
    "\n",
    "We will be using OpenAI for our embedding model and LLM, but this can easily be switched out:\n",
    "\n",
    "Various embedding functions are provided in `functions/embed.py`\n",
    "- LLM prompts are provided in `functions/llm.py`\n",
    "- NOTE: When switching out embedding models, you will need to make a new collection for your new embeddings. Then, embed the same documents and queries with the embedding model of your choice.\n",
    "\n",
    "Use the same golden dataset of queries when comparing embedding models on the same data.\n",
    "\n",
    "Cells that should be modified when switching out embedding models are labeled as **[Modify]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. (Optional) Use your own data!\n",
    "\n",
    "This demo is already preloaded with sample data from Chroma docs, but you can also use your own data.\n",
    "\n",
    "If you want to use your own data, change `COLLECTION_NAME` to the name of the collection where you want your data.\n",
    "\n",
    "If you want to create a new collection with your own local data, use `load_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data collection name: generative-benchmarking-chroma-docs\n",
    "COLLECTION_NAME = \"generative-benchmarking-chroma-docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sample dataset, we will be creating a dataset for creating a technical support bot.\n",
    "\n",
    "You can change the subject of the generated dataset by changing the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"This is a technical support bot for Chroma, a vector database company often used by developers for building AI applications.\"\n",
    "example_queries = \"\"\"\n",
    "    how to add to a collection\n",
    "    filter by metadata\n",
    "    retrieve embeddings when querying\n",
    "    how to use openai embedding function when adding to collection\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install & Import\n",
    "\n",
    "Install the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from openai import OpenAI as OpenAIClient\n",
    "from functions.utils import *\n",
    "from functions.llm import *\n",
    "from functions.embed import *\n",
    "from functions.chroma import *\n",
    "from functions.evaluate import *\n",
    "from functions.visualize import *\n",
    "from functions.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv();\n",
    "\n",
    "# Embedding Model & LLM\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY != None and len(OPENAI_API_KEY) > 0, 'Please provide an OpenAI API key'\n",
    "\n",
    "CHROMA_CLOUD_API_KEY = os.getenv(\"CHROMA_CLOUD_API_KEY\")\n",
    "CHROMA_HOST=os.getenv(\"CHROMA_HOST\")\n",
    "CHROMA_TENANT=os.getenv(\"CHROMA_TENANT\")\n",
    "CHROMA_DB_NAME=os.getenv(\"CHROMA_DB_NAME\")\n",
    "\n",
    "using_chroma_cloud = env_var_provided(CHROMA_CLOUD_API_KEY)\n",
    "\n",
    "if using_chroma_cloud:\n",
    "    assert env_var_provided(CHROMA_HOST)\n",
    "    assert env_var_provided(CHROMA_TENANT)\n",
    "    assert env_var_provided(CHROMA_DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Connect to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(\n",
    "    ssl=using_chroma_cloud,\n",
    "    host=CHROMA_HOST,\n",
    "    tenant=CHROMA_TENANT,\n",
    "    database=CHROMA_DB_NAME,\n",
    "    headers={\n",
    "        'x-chroma-token': CHROMA_CLOUD_API_KEY\n",
    "    }\n",
    ")\n",
    "\n",
    "corpus_collection = chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "assert corpus_collection.count() > 0, \\\n",
    "    ('Your collection is empty. If you want to create a new collection with '\n",
    "    'your own data, use the `load_data.py` script.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Connect to Embedding Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAIClient(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load in Your Data\n",
    "\n",
    "If you using your own data, this will embed your data. We use OpenAI's text-embedding-3-large here, but have other functions available in `embed.py`. You may also define your own embedding function.\n",
    "\n",
    "We use batching and multi-threading for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_collection_items(\n",
    "    collection=corpus_collection\n",
    ")\n",
    "corpus_ids = [key for key in corpus.keys()]\n",
    "corpus_documents = [corpus[key]['document'] for key in corpus_ids]\n",
    "corpus_embeddings = [corpus[key]['embedding'] for key in corpus_ids]\n",
    "assert len(corpus) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Documents for Quality\n",
    "\n",
    "We begin by filtering our documents prior to query generation, this step ensures that we avoid generating queries from irrelevant or incomplete documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set Criteria\n",
    "\n",
    "We use the following criteria:\n",
    "- `relevance` checks whether the document is relevant to the specified context\n",
    "- `completeness` checks for overall quality of the document\n",
    "\n",
    "You can modify the criteria as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = f\"The document is relevant to the following context: {context}\"\n",
    "completeness = \"The document is complete, meaning that it contains useful information to answer queries and does not only serve as an introduction to the main content that users may be looking for.\"\n",
    "\n",
    "criteria = [relevance, completeness]\n",
    "criteria_labels = [\"relevance\", \"completeness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filter Documents\n",
    "\n",
    "We filter our documents using gpt-4o-mini. Batching functions are also available in `llm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering documents: 100%|██████████| 648/648 [20:07<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "filtered_document_ids = filter_documents(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    documents=corpus_documents,\n",
    "    ids=corpus_ids,\n",
    "    criteria=criteria,\n",
    "    criteria_labels=criteria_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "passed_documents = [corpus[id]['document'] for id in filtered_document_ids]\n",
    "\n",
    "failed_document_ids = [id for id in corpus_ids if id not in filtered_document_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents passed: {len(filtered_document_ids)}\")\n",
    "print(f\"Number of documents failed: {len(failed_document_ids)}\")\n",
    "print(\"-\"*80)\n",
    "print(\"Example of passed document:\")\n",
    "print(corpus[filtered_document_ids[0]]['document'])\n",
    "print(\"-\"*80)\n",
    "print(\"Example of failed document:\")\n",
    "print(corpus[failed_document_ids[0]]['document'])\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Golden Dataset\n",
    "\n",
    "Using our filtered documents, we can genereate a golden dataset of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Custom Prompt\n",
    "\n",
    "We will use `context` and `example_queries` for query generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Queries\n",
    "\n",
    "Generate queries with gpt-4o. Batching functions are available in `llm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_dataset = create_golden_dataset(\n",
    "    client=openai_client,\n",
    "    model=\"gpt-4o\",\n",
    "    documents=passed_documents,\n",
    "    ids=filtered_document_ids,\n",
    "    context=context,\n",
    "    example_queries=example_queries\n",
    ")\n",
    "\n",
    "golden_collection = chroma_client.get_or_create_collection(\n",
    "    name=f'{COLLECTION_NAME}-golden',\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "golden_ids = golden_dataset['id'].tolist()\n",
    "golden_queries = golden_dataset['query'].tolist()\n",
    "golden_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=golden_queries,\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "collection_add_in_batches(\n",
    "    collection=golden_collection,\n",
    "    ids=golden_ids,\n",
    "    texts=golden_queries,\n",
    "    embeddings=golden_embeddings,\n",
    ")\n",
    "\n",
    "golden_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate\n",
    "\n",
    "Now that we have our golden dataset, we will can run our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed generated queries.\n",
    "\n",
    "**[Modify]** embedding function (`openai_embed_in_batches`) to the embedding model you wish to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = openai_embed_in_batches(\n",
    "    openai_client=openai_client,\n",
    "    texts=golden_queries,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "query_embeddings_lookup_dict = {\n",
    "    id: QueryItem(\n",
    "        text=query,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    for id, query, embedding in zip(golden_ids, golden_queries, query_embeddings)\n",
    "}\n",
    "\n",
    "query_embeddings_lookup = QueryLookup(lookup=query_embeddings_lookup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our qrels (query relevance labels) dataframe. In this case, each query and its corresponding document share the same id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = pd.DataFrame(\n",
    "    {\n",
    "        \"query-id\": golden_ids,\n",
    "        \"corpus-id\": golden_ids,\n",
    "        \"score\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_benchmark(\n",
    "    query_embeddings_lookup=query_embeddings_lookup,\n",
    "    collection=corpus_collection,\n",
    "    qrels=qrels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results.\n",
    "\n",
    "This is helpful for comparison (e.g. comparing different embedding models).\n",
    "\n",
    "**[Modify]** \"model\" to the model you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "results_to_save = {\n",
    "    \"model\": \"text-embedding-3-large\",\n",
    "    \"results\": results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"results\")\n",
    "\n",
    "with open(os.path.join(results_dir, f'{timestamp}.json'), 'w') as f:\n",
    "    json.dump(results_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished\n",
    "\n",
    "Your dataset is ready!\n",
    "\n",
    "Next steps:\n",
    "1. Rerun this notebook using different embedding models\n",
    "2. Use `compare.ipynb` to compare the retrieval performance of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'View your dataset: `chroma browse {COLLECTION_NAME}{' --local' if not using_chroma_cloud else ''}`')\n",
    "print(f'View the generated golden dataset: `chroma browse {COLLECTION_NAME}-golden{(' --local' if not using_chroma_cloud else '')}`')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-benchmarking-16hOiizI-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
