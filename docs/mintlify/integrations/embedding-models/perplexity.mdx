---
title: Perplexity
---

Chroma provides a convenient wrapper around Perplexity's embedding API. This embedding function runs remotely on Perplexity's servers, and requires an API key. You can get an API key by signing up for an account at [Perplexity](https://www.perplexity.ai/).

<Tabs>
<Tab title="Python" icon="python">

This embedding function relies on the `perplexityai` python package, which you can install with `pip install perplexityai`.

```python
import chromadb.utils.embedding_functions as embedding_functions

perplexity_ef = embedding_functions.PerplexityEmbeddingFunction(
    api_key="YOUR_API_KEY",
    model_name="pplx-embed-v1-4b"
)

perplexity_ef(input=["document1", "document2"])
```

</Tab>
<Tab title="TypeScript" icon="js">

```typescript
// npm install @chroma-core/perplexity

import { PerplexityEmbeddingFunction } from "@chroma-core/perplexity";

const embedder = new PerplexityEmbeddingFunction({
    apiKey: "YOUR_API_KEY",
    modelName: "pplx-embed-v1-4b",
});

// use directly
const embeddings = await embedder.generate(["document1", "document2"]);

// pass documents to query for .add and .query
const collection = await client.createCollection({
    name: "name",
    embeddingFunction: embedder,
});
const collectionGet = await client.getCollection({
    name: "name",
    embeddingFunction: embedder,
});
```

</Tab>
</Tabs>

## Semantic Search with Chroma

Here's a complete example of using Perplexity embeddings with Chroma for semantic search:

<Tabs>
<Tab title="Python" icon="python">

```python
import chromadb
import chromadb.utils.embedding_functions as embedding_functions

# Initialize the embedding function
perplexity_ef = embedding_functions.PerplexityEmbeddingFunction(
    api_key="YOUR_API_KEY",
    model_name="pplx-embed-v1-4b"
)

# Create a Chroma client and collection
client = chromadb.Client()
collection = client.create_collection(
    name="my_documents",
    embedding_function=perplexity_ef
)

# Add documents
documents = [
    "Python is a versatile programming language",
    "Machine learning automates analytical model building",
    "The Eiffel Tower is located in Paris, France"
]

collection.add(
    documents=documents,
    ids=["doc1", "doc2", "doc3"]
)

# Query for similar documents
results = collection.query(
    query_texts=["What programming languages are good for data science?"],
    n_results=2
)

print("Search results:")
for doc, distance in zip(results["documents"][0], results["distances"][0]):
    print(f"  {distance:.4f}: {doc}")
```

</Tab>
<Tab title="TypeScript" icon="js">

```typescript
import { ChromaClient } from "chromadb";
import { PerplexityEmbeddingFunction } from "@chroma-core/perplexity";

// Initialize the embedding function
const perplexityEf = new PerplexityEmbeddingFunction({
    apiKey: "YOUR_API_KEY",
    modelName: "pplx-embed-v1-4b",
});

// Create a Chroma client and collection
const client = new ChromaClient();
const collection = await client.createCollection({
    name: "my_documents",
    embeddingFunction: perplexityEf,
});

// Add documents
const documents = [
    "Python is a versatile programming language",
    "Machine learning automates analytical model building",
    "The Eiffel Tower is located in Paris, France",
];

await collection.add({
    documents: documents,
    ids: ["doc1", "doc2", "doc3"],
});

// Query for similar documents
const results = await collection.query({
    queryTexts: ["What programming languages are good for data science?"],
    nResults: 2,
});

console.log("Search results:");
results.documents[0].forEach((doc, i) => {
    console.log(`  ${results.distances[0][i].toFixed(4)}: ${doc}`);
});
```

</Tab>
</Tabs>

## Available Models

Perplexity offers two embedding models:

| Model | Dimensions | Context Window | Price |
|-------|------------|----------------|-------|
| `pplx-embed-v1-0.6b` | 1024 | 32K tokens | $0.004/1M tokens |
| `pplx-embed-v1-4b` | 2560 | 32K tokens | $0.03/1M tokens |

## Matryoshka Dimensions

Both models support [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147), allowing you to reduce embedding dimensions while maintaining quality. This is useful for reducing storage costs and improving search speed.

<Tabs>
<Tab title="Python" icon="python">

```python
# Reduce dimensions from 2560 to 512 for the 4b model
perplexity_ef = embedding_functions.PerplexityEmbeddingFunction(
    api_key="YOUR_API_KEY",
    model_name="pplx-embed-v1-4b",
    dimensions=512
)

embeddings = perplexity_ef(input=["document1", "document2"])
print(len(embeddings[0]))  # 512
```

</Tab>
<Tab title="TypeScript" icon="js">

```typescript
// Reduce dimensions from 2560 to 512 for the 4b model
const embedder = new PerplexityEmbeddingFunction({
    apiKey: "YOUR_API_KEY",
    modelName: "pplx-embed-v1-4b",
    dimensions: 512,
});

const embeddings = await embedder.generate(["document1", "document2"]);
console.log(embeddings[0].length);  // 512
```

</Tab>
</Tabs>

Supported dimension ranges:
- `pplx-embed-v1-0.6b`: 128 to 1024
- `pplx-embed-v1-4b`: 128 to 2560

For more details on Perplexity's embedding models, check the [documentation](https://docs.perplexity.ai/docs/embeddings/standard-embeddings).