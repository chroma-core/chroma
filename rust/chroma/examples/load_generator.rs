//! Load Generator Example
//!
//! A load generator for Chroma that creates concurrent upsert operations across multiple
//! collections on two different Chroma endpoints.
//!
//! # Features
//!
//! - Dual endpoint support (api.trychroma.com and europe-west1.gcp.devchroma.com)
//! - Configurable number of collections, tasks, batch size, and duration
//! - Round-robin collection selection within each task
//! - Gaussian Mixture Model (GMM) for realistic embedding generation
//!
//! # Usage
//!
//! ```bash
//! cargo run --example load_generator -- --collections 10 --duration 600 --tasks 4 --batch-size 100
//! ```
//!
//! # Environment Variables
//!
//! The following environment variables must be set:
//! - `CHROMA_API_KEY` - API key for Chroma Cloud authentication
//! - `CHROMA_TENANT` - Tenant ID (optional, will be auto-resolved)
//! - `CHROMA_DATABASE` - Database name (optional, will be auto-resolved)

use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};

use chroma::client::ChromaHttpClientOptions;
use chroma::ChromaCollection;
use chroma::ChromaHttpClient;
use clap::Parser;
use rand::rngs::StdRng;
use rand::{Rng, SeedableRng};
use tokio::sync::{mpsc, Mutex};

/// Default embedding dimension for the GMM.
const EMBEDDING_DIM: usize = 1536;

/// Number of clusters in the Gaussian Mixture Model.
const NUM_CLUSTERS: usize = 1000;

/// Gaussian Mixture Model for generating realistic embeddings.
///
/// The model consists of cluster centroids and covariance parameters.
/// Embeddings are generated by sampling from a randomly selected cluster.
struct GaussianMixtureModel {
    /// Cluster centroids, each of dimension EMBEDDING_DIM.
    centroids: Vec<Vec<f32>>,
    /// Standard deviation for each cluster.
    std_devs: Vec<f32>,
}

impl GaussianMixtureModel {
    /// Creates a new Gaussian Mixture Model with the specified parameters.
    ///
    /// The model is initialized with randomly placed centroids and varying
    /// standard deviations to create a diverse embedding space.
    fn new(seed: u64) -> Self {
        let mut rng = StdRng::seed_from_u64(seed);

        // Generate cluster centroids
        let centroids: Vec<Vec<f32>> = (0..NUM_CLUSTERS)
            .map(|_| {
                (0..EMBEDDING_DIM)
                    .map(|_| rng.gen_range(-1.0..1.0))
                    .collect()
            })
            .collect();

        // Generate varying standard deviations for each cluster
        let std_devs: Vec<f32> = (0..NUM_CLUSTERS)
            .map(|_| rng.gen_range(0.01..0.1))
            .collect();

        Self {
            centroids,
            std_devs,
        }
    }

    /// Generates a batch of embeddings by sampling from the mixture model.
    fn generate_batch(&self, rng: &mut StdRng, batch_size: usize) -> Vec<Vec<f32>> {
        (0..batch_size)
            .map(|_| {
                // Select a random cluster
                let cluster_idx = rng.gen_range(0..NUM_CLUSTERS);
                let centroid = &self.centroids[cluster_idx];
                let std_dev = self.std_devs[cluster_idx];

                // Sample from the cluster using Box-Muller transform for Gaussian noise
                centroid
                    .iter()
                    .map(|&c| {
                        let u1: f32 = rng.gen_range(0.0001..1.0);
                        let u2: f32 = rng.gen_range(0.0..1.0);
                        let z = (-2.0 * u1.ln()).sqrt() * (2.0 * std::f32::consts::PI * u2).cos();
                        c + z * std_dev
                    })
                    .collect()
            })
            .collect()
    }
}

/// Load generator for Chroma that creates concurrent upsert operations.
#[derive(Parser, Debug)]
#[command(name = "load_generator")]
#[command(about = "Generate load against Chroma endpoints")]
struct Args {
    /// Number of collections to create and write to.
    #[arg(short, long, default_value_t = 10)]
    collections: usize,

    /// Duration to run the load generator in seconds.
    #[arg(short, long, default_value_t = 600)]
    duration: u64,

    /// Number of concurrent tasks.
    #[arg(short, long, default_value_t = 4)]
    tasks: usize,

    /// Batch size for upsert operations.
    #[arg(short, long, default_value_t = 100)]
    batch_size: usize,

    /// Target request pace in queries per second.
    #[arg(long, default_value_t = 100)]
    pace_qps: u64,
}

/// Shared state for worker tasks.
struct WorkerContext {
    gmm: Arc<GaussianMixtureModel>,
    total_upserts: Arc<AtomicU64>,
    total_records: Arc<AtomicU64>,
    batch_size: usize,
    start_time: Instant,
    duration: Duration,
    pacing_rx: Arc<Mutex<mpsc::Receiver<()>>>,
}

/// Creates a client with the specified base URL, using credentials from environment.
fn create_client(base_url: &str) -> Result<ChromaHttpClient, Box<dyn std::error::Error>> {
    let mut options = ChromaHttpClientOptions::from_cloud_env()?;
    options.endpoint = base_url.parse()?;
    Ok(ChromaHttpClient::new(options))
}

/// Generates a deterministic collection name from the index.
fn collection_name(index: usize) -> String {
    format!("loadgen_collection_{:06}", index)
}

/// Runs a worker task that performs upserts in a round-robin fashion across collections.
async fn run_worker(
    collections: Vec<ChromaCollection>,
    ctx: WorkerContext,
    seed: u64,
    id_prefix: String,
) {
    let mut rng = StdRng::seed_from_u64(seed);
    let mut record_counter: u64 = 0;
    let mut collection_idx: usize = 0;
    let num_collections = collections.len();

    while ctx.start_time.elapsed() < ctx.duration {
        let remaining = ctx.duration.saturating_sub(ctx.start_time.elapsed());
        if remaining.is_zero() {
            break;
        }

        let ticket = tokio::time::timeout(remaining, async {
            let mut rx = ctx.pacing_rx.lock().await;
            rx.recv().await
        })
        .await;

        match ticket {
            Ok(Some(())) => {}
            _ => break,
        }

        // Round-robin collection selection
        let collection = &collections[collection_idx];
        collection_idx = (collection_idx + 1) % num_collections;

        // Generate batch
        let embeddings = ctx.gmm.generate_batch(&mut rng, ctx.batch_size);
        let ids: Vec<String> = (0..ctx.batch_size)
            .map(|i| {
                record_counter += 1;
                format!("{}_{}", id_prefix, record_counter + i as u64)
            })
            .collect();

        // Perform upsert
        match collection.upsert(ids, embeddings, None, None, None).await {
            Ok(_) => {
                ctx.total_upserts.fetch_add(1, Ordering::Relaxed);
                ctx.total_records
                    .fetch_add(ctx.batch_size as u64, Ordering::Relaxed);
            }
            Err(e) => {
                eprintln!("[{}] Upsert error: {}", id_prefix, e);
            }
        }
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();

    println!("=== Chroma Load Generator ===");
    println!("Collections: {}", args.collections);
    println!("Duration: {} seconds", args.duration);
    println!("Tasks: {}", args.tasks);
    println!("Batch size: {}", args.batch_size);
    println!("Pace: {} qps", args.pace_qps.max(1));
    println!();

    // Create clients for both endpoints
    let client_us = create_client("https://api.trychroma.com:443")?;
    let client_eu = create_client("https://europe-west1.gcp.devchroma.com:443")?;

    println!(
        "Creating/getting {} collections on both endpoints...",
        args.collections
    );

    // Create or get collections on both endpoints
    let mut collections_us: Vec<ChromaCollection> = Vec::with_capacity(args.collections);
    let mut collections_eu: Vec<ChromaCollection> = Vec::with_capacity(args.collections);

    for i in 0..args.collections {
        let name = collection_name(i);

        let collection_us = client_us
            .get_or_create_collection(&name, None, None)
            .await?;
        collections_us.push(collection_us);

        let collection_eu = client_eu
            .get_or_create_collection(&name, None, None)
            .await?;
        collections_eu.push(collection_eu);

        if (i + 1) % 10 == 0 || i == args.collections - 1 {
            println!("  Created {}/{} collections", i + 1, args.collections);
        }
    }

    println!("Collections ready. Starting load generation...\n");

    // Shared state
    let gmm = Arc::new(GaussianMixtureModel::new(42));
    let total_upserts = Arc::new(AtomicU64::new(0));
    let total_records = Arc::new(AtomicU64::new(0));

    let start_time = Instant::now();
    let duration = Duration::from_secs(args.duration);
    let pace_qps = args.pace_qps.max(1);
    let ticket_interval = Duration::from_secs_f64(1.0 / pace_qps as f64);

    let (ticket_tx, ticket_rx) = mpsc::channel::<()>(1024);
    let pacing_rx = Arc::new(Mutex::new(ticket_rx));

    let pacing_handle = {
        let start_time = start_time;
        let duration = duration;
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(ticket_interval);
            while start_time.elapsed() < duration {
                interval.tick().await;
                let _ = ticket_tx.try_send(());
            }
        })
    };

    // Spawn worker tasks
    let mut handles = Vec::new();

    for task_id in 0..args.tasks {
        // US endpoint task
        let ctx = WorkerContext {
            gmm: Arc::clone(&gmm),
            total_upserts: Arc::clone(&total_upserts),
            total_records: Arc::clone(&total_records),
            batch_size: args.batch_size,
            start_time,
            duration,
            pacing_rx: Arc::clone(&pacing_rx),
        };
        let handle = tokio::spawn(run_worker(
            collections_us.clone(),
            ctx,
            task_id as u64 * 1000,
            format!("us_task{}", task_id),
        ));
        handles.push(handle);

        // EU endpoint task
        let ctx = WorkerContext {
            gmm: Arc::clone(&gmm),
            total_upserts: Arc::clone(&total_upserts),
            total_records: Arc::clone(&total_records),
            batch_size: args.batch_size,
            start_time,
            duration,
            pacing_rx: Arc::clone(&pacing_rx),
        };
        let handle = tokio::spawn(run_worker(
            collections_eu.clone(),
            ctx,
            (task_id as u64 + 500) * 1000,
            format!("eu_task{}", task_id),
        ));
        handles.push(handle);
    }

    // Progress reporting task
    let total_upserts_report = Arc::clone(&total_upserts);
    let total_records_report = Arc::clone(&total_records);
    let report_handle = tokio::spawn(async move {
        let mut last_upserts = 0u64;
        let mut last_records = 0u64;
        let report_interval = Duration::from_secs(10);

        while start_time.elapsed() < duration {
            tokio::time::sleep(report_interval).await;

            let current_upserts = total_upserts_report.load(Ordering::Relaxed);
            let current_records = total_records_report.load(Ordering::Relaxed);
            let elapsed = start_time.elapsed().as_secs_f64();

            let upserts_delta = current_upserts - last_upserts;
            let records_delta = current_records - last_records;

            println!(
                "[{:.0}s] Total: {} upserts, {} records | Rate: {:.1} upserts/s, {:.1} records/s",
                elapsed,
                current_upserts,
                current_records,
                upserts_delta as f64 / report_interval.as_secs_f64(),
                records_delta as f64 / report_interval.as_secs_f64()
            );

            last_upserts = current_upserts;
            last_records = current_records;
        }
    });

    // Wait for all tasks to complete
    for handle in handles {
        let _ = handle.await;
    }
    report_handle.abort();
    pacing_handle.abort();

    let elapsed = start_time.elapsed();
    let final_upserts = total_upserts.load(Ordering::Relaxed);
    let final_records = total_records.load(Ordering::Relaxed);

    println!("\n=== Load Generation Complete ===");
    println!("Duration: {:.1} seconds", elapsed.as_secs_f64());
    println!("Total upserts: {}", final_upserts);
    println!("Total records: {}", final_records);
    println!(
        "Average rate: {:.1} upserts/s, {:.1} records/s",
        final_upserts as f64 / elapsed.as_secs_f64(),
        final_records as f64 / elapsed.as_secs_f64()
    );

    Ok(())
}
